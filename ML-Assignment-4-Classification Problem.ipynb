{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a799badb-9262-4c43-8214-0c61433c1f11",
   "metadata": {},
   "source": [
    "### **ASSIGNMENT 4 - CLASSIFICATION** ###\n",
    "**Objective:**\n",
    "The goal of this assessment is to test your understanding and ability to apply supervised learning techniques to a real-world dataset.\n",
    "\n",
    "**Dataset:**\n",
    "Use the breast cancer dataset available in the sklearn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d554830d-917b-4843-9e30-47d70754182d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import sys\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f18be2c-9c2e-4038-ac0f-e7231b1df876",
   "metadata": {},
   "source": [
    "## 1. Loading and Preprocessing ##\n",
    "● Load the breast cancer dataset from sklearn.\n",
    "\n",
    "● Preprocess the data to handle any missing values and perform necessary feature scaling.\n",
    "\n",
    "● Explain the preprocessing steps you performed and justify why they are necessary for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5554aebd",
   "metadata": {},
   "source": [
    "import warnings\n",
    "import sys\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")import warnings\n",
    "import sys\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37199034-1143-4a31-b9ef-fda682348144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ee91283-af72-4457-9f4a-c47575bbb9e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (569, 30)\n",
      "Number of missing values in each column:\n",
      " mean radius                0\n",
      "mean texture               0\n",
      "mean perimeter             0\n",
      "mean area                  0\n",
      "mean smoothness            0\n",
      "mean compactness           0\n",
      "mean concavity             0\n",
      "mean concave points        0\n",
      "mean symmetry              0\n",
      "mean fractal dimension     0\n",
      "radius error               0\n",
      "texture error              0\n",
      "perimeter error            0\n",
      "area error                 0\n",
      "smoothness error           0\n",
      "compactness error          0\n",
      "concavity error            0\n",
      "concave points error       0\n",
      "symmetry error             0\n",
      "fractal dimension error    0\n",
      "worst radius               0\n",
      "worst texture              0\n",
      "worst perimeter            0\n",
      "worst area                 0\n",
      "worst smoothness           0\n",
      "worst compactness          0\n",
      "worst concavity            0\n",
      "worst concave points       0\n",
      "worst symmetry             0\n",
      "worst fractal dimension    0\n",
      "dtype: int64\n",
      "\n",
      "First 5 rows of scaled features:\n",
      "    mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
      "0     1.097064     -2.073335        1.269934   0.984375         1.568466   \n",
      "1     1.829821     -0.353632        1.685955   1.908708        -0.826962   \n",
      "2     1.579888      0.456187        1.566503   1.558884         0.942210   \n",
      "3    -0.768909      0.253732       -0.592687  -0.764464         3.283553   \n",
      "4     1.750297     -1.151816        1.776573   1.826229         0.280372   \n",
      "\n",
      "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
      "0          3.283515        2.652874             2.532475       2.217515   \n",
      "1         -0.487072       -0.023846             0.548144       0.001392   \n",
      "2          1.052926        1.363478             2.037231       0.939685   \n",
      "3          3.402909        1.915897             1.451707       2.867383   \n",
      "4          0.539340        1.371011             1.428493      -0.009560   \n",
      "\n",
      "   mean fractal dimension  ...  worst radius  worst texture  worst perimeter  \\\n",
      "0                2.255747  ...      1.886690      -1.359293         2.303601   \n",
      "1               -0.868652  ...      1.805927      -0.369203         1.535126   \n",
      "2               -0.398008  ...      1.511870      -0.023974         1.347475   \n",
      "3                4.910919  ...     -0.281464       0.133984        -0.249939   \n",
      "4               -0.562450  ...      1.298575      -1.466770         1.338539   \n",
      "\n",
      "   worst area  worst smoothness  worst compactness  worst concavity  \\\n",
      "0    2.001237          1.307686           2.616665         2.109526   \n",
      "1    1.890489         -0.375612          -0.430444        -0.146749   \n",
      "2    1.456285          0.527407           1.082932         0.854974   \n",
      "3   -0.550021          3.394275           3.893397         1.989588   \n",
      "4    1.220724          0.220556          -0.313395         0.613179   \n",
      "\n",
      "   worst concave points  worst symmetry  worst fractal dimension  \n",
      "0              2.296076        2.750622                 1.937015  \n",
      "1              1.087084       -0.243890                 0.281190  \n",
      "2              1.955000        1.152255                 0.201391  \n",
      "3              2.175786        6.046041                 4.935010  \n",
      "4              0.729259       -0.868353                -0.397100  \n",
      "\n",
      "[5 rows x 30 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load the breast cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target, name=\"target\")\n",
    "\n",
    "# Display basic info about the dataset\n",
    "print(\"Dataset shape:\", X.shape)\n",
    "print(\"Number of missing values in each column:\\n\", X.isnull().sum())\n",
    "\n",
    "# Preprocessing: Handling missing values\n",
    "# In this dataset, there are no missing values. However, in general, we can handle missing values as follows:\n",
    "# Example: X.fillna(X.mean(), inplace=True)  # Replace missing values with column mean\n",
    "\n",
    "# Preprocessing: Feature scaling\n",
    "scaler = StandardScaler()  # Standardization scales the features to have mean=0 and variance=1\n",
    "X_scaled = scaler.fit_transform(X)  # Fit and transform the features\n",
    "\n",
    "# Splitting the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Verify scaled data\n",
    "print(\"\\nFirst 5 rows of scaled features:\\n\", pd.DataFrame(X_scaled, columns=data.feature_names).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ff618b-26fc-4191-8c93-dfd219ac3b85",
   "metadata": {},
   "source": [
    "### 2. Classification Algorithm Implementation ###\n",
    "Implement the following five classification algorithms:\n",
    "\n",
    "Logistic Regression\n",
    "Decision Tree Classifier\n",
    "Random Forest Classifier\n",
    "Support Vector Machine (SVM)\n",
    "k-Nearest Neighbors (k-NN)\n",
    "For each algorithm, provide a brief description of how it works and why it may be suitable for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "501ab4ad-4ebb-426e-b957-e2c7316bbb5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Logistic Regression ===\n",
      "Accuracy: 97.37%\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.95      0.96        43\n",
      "           1       0.97      0.99      0.98        71\n",
      "\n",
      "    accuracy                           0.97       114\n",
      "   macro avg       0.97      0.97      0.97       114\n",
      "weighted avg       0.97      0.97      0.97       114\n",
      "\n",
      "\n",
      "=== Decision Tree ===\n",
      "Accuracy: 94.74%\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93        43\n",
      "           1       0.96      0.96      0.96        71\n",
      "\n",
      "    accuracy                           0.95       114\n",
      "   macro avg       0.94      0.94      0.94       114\n",
      "weighted avg       0.95      0.95      0.95       114\n",
      "\n",
      "\n",
      "=== Random Forest ===\n",
      "Accuracy: 96.49%\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.93      0.95        43\n",
      "           1       0.96      0.99      0.97        71\n",
      "\n",
      "    accuracy                           0.96       114\n",
      "   macro avg       0.97      0.96      0.96       114\n",
      "weighted avg       0.97      0.96      0.96       114\n",
      "\n",
      "\n",
      "=== Support Vector Machine (SVM) ===\n",
      "Accuracy: 95.61%\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.95      0.94        43\n",
      "           1       0.97      0.96      0.96        71\n",
      "\n",
      "    accuracy                           0.96       114\n",
      "   macro avg       0.95      0.96      0.95       114\n",
      "weighted avg       0.96      0.96      0.96       114\n",
      "\n",
      "\n",
      "=== k-Nearest Neighbors (k-NN) ===\n",
      "Accuracy: 94.74%\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93        43\n",
      "           1       0.96      0.96      0.96        71\n",
      "\n",
      "    accuracy                           0.95       114\n",
      "   macro avg       0.94      0.94      0.94       114\n",
      "weighted avg       0.95      0.95      0.95       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Dictionary to store classifiers and their names\n",
    "classifiers = {\n",
    "    \"Logistic Regression\": LogisticRegression(random_state=42),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42, n_estimators=100),\n",
    "    \"Support Vector Machine (SVM)\": SVC(kernel='linear', random_state=42),\n",
    "    \"k-Nearest Neighbors (k-NN)\": KNeighborsClassifier(n_neighbors=5)\n",
    "}\n",
    "\n",
    "# Loop through each classifier, train it, and evaluate it\n",
    "for name, clf in classifiers.items():\n",
    "    # Train the classifier\n",
    "    clf.fit(X_train, y_train)\n",
    "    # Make predictions\n",
    "    y_pred = clf.predict(X_test)\n",
    "    # Evaluate performance\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(\"Accuracy: {:.2f}%\".format(accuracy_score(y_test, y_pred) * 100))\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6daa5d-89e9-427b-b130-8de282c680dd",
   "metadata": {},
   "source": [
    "## Explanation of Each Classification Algorithm:\n",
    "## 1. Logistic Regression: \n",
    "\n",
    "**How it works:**  Logistic regression is used for binary classification. It calculates the probability of an instance belonging to a class using the sigmoid function and makes predictions based on a decision boundary (typically 0.5).\n",
    "Suitability: It works well for binary classification problems (malignant or benign) and is effective when features have a linear relationship with the target.\n",
    "## 2. Decision Tree Classifier: \n",
    "\n",
    "**How it works:** A decision tree splits the data into subsets based on feature values, creating a tree structure. Each node represents a feature, and branches represent decisions leading to leaf nodes (classes).\n",
    "Suitability: Decision trees handle non-linear relationships well and are interpretable, making them suitable for datasets with complex feature interactions like the breast cancer dataset.\n",
    "## 3. Random Forest Classifier:\n",
    "\n",
    "**How it works:** A random forest combines multiple decision trees trained on random subsets of data and aggregates their predictions through majority voting.\n",
    "Suitability: Random forests reduce overfitting (common in individual decision trees) and improve generalization, making them effective for datasets with noise and feature interactions.\n",
    "## 4. Support Vector Machine (SVM):\n",
    "\n",
    "**How it works:** SVM creates a hyperplane that best separates the data into classes by maximizing the margin between the closest points of each class (support vectors).\n",
    "Suitability: SVM is effective for datasets with a clear margin of separation between classes and performs well with high-dimensional data like the breast cancer dataset.\n",
    "## 5. k-Nearest Neighbors (k-NN):\n",
    "\n",
    "**How it works:** k-NN classifies an instance based on the majority class among its k nearest neighbors in the feature space.\n",
    "Suitability: k-NN is non-parametric and works well for datasets where the decision boundary is not linear. It can capture complex patterns in the breast cancer dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868756e1-2a5c-4fa4-8924-2616a7baaaf0",
   "metadata": {},
   "source": [
    "## **3. Model Comparison**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f94636-24db-4b03-9de2-97e26e94cf90",
   "metadata": {},
   "source": [
    "**Comparison of Classification Algorithms**\n",
    "\n",
    "## 1. Logistic Regression:\n",
    "\n",
    "Accuracy: 97.37%\n",
    "Precision: High for both classes (0 = 98%, 1 = 97%)\n",
    "Recall: High for both classes (0 = 95%, 1 = 99%)\n",
    "F1-Score: 96% (Class 0), 98% (Class 1)\n",
    "Observations: Logistic regression performed the best with the highest accuracy and strong performance across all metrics. It is well-suited for binary classification problems with linearly separable data.\n",
    "## 2. Decision Tree:\n",
    "\n",
    "Accuracy: 94.74%\n",
    "Precision: Good for both classes (0 = 93%, 1 = 96%)\n",
    "Recall: Good for both classes (0 = 93%, 1 = 96%)\n",
    "F1-Score: 93% (Class 0), 96% (Class 1)\n",
    "Observations: Decision trees performed decently but had slightly lower accuracy. Overfitting might be an issue, which reduces their performance compared to other models.\n",
    "## 3. Random Forest:\n",
    "\n",
    "Accuracy: 96.49%\n",
    "Precision: High for both classes (0 = 98%, 1 = 96%)\n",
    "Recall: High for both classes (0 = 93%, 1 = 99%)\n",
    "F1-Score: 95% (Class 0), 97% (Class 1)\n",
    "Observations: Random forests performed almost as well as logistic regression. They reduced overfitting and handled feature interactions well, but had slightly lower recall for Class 0.\n",
    "## 4. Support Vector Machine (SVM):\n",
    "\n",
    "Accuracy: 95.61%\n",
    "Precision: Good for both classes (0 = 93%, 1 = 97%)\n",
    "Recall: Good for both classes (0 = 95%, 1 = 96%)\n",
    "F1-Score: 94% (Class 0), 96% (Class 1)\n",
    "Observations: SVM performed well with good accuracy, but it was slightly less effective than logistic regression and random forests.\n",
    "## 5. k-Nearest Neighbors (k-NN):\n",
    "\n",
    "Accuracy: 94.74%\n",
    "Precision: Good for both classes (0 = 93%, 1 = 96%)\n",
    "Recall: Good for both classes (0 = 93%, 1 = 96%)\n",
    "F1-Score: 93% (Class 0), 96% (Class 1)\n",
    "Observations: k-NN performed similarly to decision trees with slightly lower accuracy and F1-scores. It may be sensitive to the choice of k and feature scaling.\n",
    "Best and Worst-Performing Algorithms:\n",
    "Best-Performing Algorithm: Logistic Regression\n",
    "Logistic regression achieved the highest accuracy (97.37%) and strong metrics across precision, recall, and F1-scores, making it the most effective model for this dataset.\n",
    "\n",
    "**Worst-Performing Algorithm: Decision Tree (and k-NN)\n",
    "Both decision tree and k-NN had the lowest accuracy (94.74%). Decision trees may have suffered from overfitting, while k-NN could have struggled due to the choice of k and distance-based classification.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
